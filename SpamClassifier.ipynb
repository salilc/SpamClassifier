{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/salil/Downloads/spam.csv',encoding='latin-1')\n",
    "\n",
    "df = df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
    "df = df.rename(columns = {'v1':'spam','v2':'message'})\n",
    "df['spam'] = df[\"spam\"].map({'spam':1,'ham':0})\n",
    "df['length'] = df['message'].apply(len)\n",
    "\n",
    "text_message = df['message'].copy()\n",
    "ham_words = ''\n",
    "spam_words = ''\n",
    "df_spam = df[df.spam == 1]\n",
    "df_ham = df[df.spam ==0]\n",
    "\n",
    "\n",
    "for val in df_spam.message:\n",
    "    text = val.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for words in tokens:\n",
    "        spam_words = spam_words + words + ' '\n",
    "        \n",
    "for val in df_ham.message:\n",
    "    text = val.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for words in tokens:\n",
    "        ham_words = ham_words + words + ' '\n",
    "# Generate a word cloud image\n",
    "spam_wordcloud = WordCloud(width=600, height=400).generate(spam_words)\n",
    "ham_wordcloud = WordCloud(width=600, height=400).generate(ham_words)\n",
    "#Spam Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(spam_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Ham Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(ham_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    return \" \".join(text)\n",
    "\n",
    "text_process = text_message.apply(text_preprocess)\n",
    "print text_process\n",
    "\n",
    "def stemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    porter = PorterStemmer()\n",
    "    for i in text:\n",
    "        #unicode(i, errors='replace')\n",
    "        words += (porter.stem(i))+\" \"\n",
    "    return words\n",
    "text_stem = text_process.apply(stemmer)\n",
    "print text_stem\n",
    "\n",
    "def lemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    lem = WordNetLemmatizer()\n",
    "    for i in text:\n",
    "        #unicode(i, errors='replace')\n",
    "        words += (lem.lemmatize(i))+\" \"\n",
    "    return words\n",
    "text_lem = text_stem.apply(lemmer)\n",
    "print text_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\"english\")\n",
    "features = vectorizer.fit_transform(text_lem)\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, df['spam'], test_size=0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier(n_neighbors=49)\n",
    "nbc = MultinomialNB(alpha=0.2)\n",
    "dtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=31, random_state=111)\n",
    "abc = AdaBoostClassifier(n_estimators=62, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'SVC' : svc,'KNN' : knc, 'NB': nbc, 'DT': dtc, 'LR': lrc, 'RF': rfc, 'AdaBoost': abc}\n",
    "\n",
    "def train_classifier(model, feature_train, labels_train):    \n",
    "    model.fit(feature_train, labels_train)\n",
    "def predict_labels(model, features):\n",
    "    return (model.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores = []\n",
    "for k,v in models.items():\n",
    "    train_classifier(v, features_train, labels_train)\n",
    "    pred = predict_labels(v,features_test)\n",
    "    pred_scores.append((k, [accuracy_score(labels_test,pred)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame(pred_scores, columns=['Classifier','Score'])\n",
    "print df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
